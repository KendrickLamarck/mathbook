created: 20200702195420281
creator: Leander
modified: 20230923143329463
modifier: Leander
revision: 0
tags: Stochastik [[Machine learning]]
title: Relative Entropie
type: text/vnd.tiddlywiki

Seien $$\prob$$ und $$\mathrm{Q}$$ Wahrscheinlichkeitsmaße. Dann ist die //relative Entropie// oder //Kullback-Leibler-Divergenz// definiert als
$$
\Eta(\,\prob\mid\mathrm{Q}\,) \coloneqq
\expect_\prob{\left( \log\fracderiv{\prob}{\mathrm{Q}} \right)}
$$
falls die [[Radon-Nikodym-Ableitung|Dichte]] existiert. Besitzen $$\prob$$ und $$\mathrm{Q}$$ die Lebesgue-Dichten $$\rho$$ und $$\sigma$$, so können wir das auch schreiben als
$$
\int_\R \rho(x) \log\frac{\rho(x)}{\sigma(x)}\,\d x.
$$

!! In machine learning

We write this as $$\dKL(\mu \mmid \nu).$$ We have $$\dKL(\mu \mmid \nu) \ge 0$$ with $$\dKL(\mu \mmid \nu) = 0$$ if and only if $$\mu = \nu$$.