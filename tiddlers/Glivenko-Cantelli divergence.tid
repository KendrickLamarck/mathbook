created: 20230729225705977
modified: 20230729230505825
tags: [[Machine learning]]
title: Glivenko-Cantelli divergence

Let $$\mathcal{F}_\mathrm{GC} = \{\,g=\mathbf{1}_{\lopenI{-\infty, a}} : a \in \R^d \,\}$$ be the set of indicator functinos on left open intervals on $$\R^d,$$ i.e. $$\lopenI{-\infty, a} = \{\, x = (x_1,...,x_d)^\transp \in \R^d : x_j \le a_j \forall j \,\}.$$ The Glivenko-Cantelli divergence is then defined as
$$
\mathsf{d}_\mathrm{GC}(\mu, \nu) =
\mathsf{d}_{\mathcal{F}_\mathrm{GC}} =
\sup_{a \in \R^d} |F_\mu(a) - F_\nu(a)|,
$$
where these cumulative distribution functions are defined as in the 1-dimensional case with the $$d$$-dimensional intervals.

This is a metric on $$\probmeasures(\R^d).$$