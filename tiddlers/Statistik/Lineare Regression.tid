created: 20200630131243543
modified: 20210118214654293
revision: 0
tags: [[Lineares Modell]]
title: Lineare Regression
type: text/vnd.tiddlywiki

Angenommen, wir haben zufällige Messwerte $$X = (X_1,\dots,X_n)$$ bei Eingangsgrößen (Zeit, Temperatur,...) $$t = (t_1,\dots,t_n).$$ Wir wollen dann eine Darstellung der folgenden Form für $$X_k$$ finden:
$$
X_k = \gamma_0 + \gamma_1 t_k + \sqrt{v} \xi_k,
$$
mit
$$\gamma_0,\gamma_1 \in \R,\ v>0$$
unbekannt und geeigneten Zufallsvariablen
$$\xi = (\xi_1,\dots,\xi_n)$$ mit $$\expect[\xi_k] = 0,\ \Var[\xi_k] = 1$$ für $$1 \le k \le n.$$
In Vektorieller Form ergibt sich
$$
X = \gamma_0\vec{1} + \gamma_1 t + \sqrt{v} \xi.
$$
Das statistische Modell ist also
$$(\R^n,\Borel_n,\prob_{\gamma,v}:(\gamma,v) \in \R \times \openI{0,\infty}),$$ wobei $$\prob_{\gamma,v}$$ die Verteilung von $$X$$ beschreibt.

!! Prinzip der kleinsten Quadrate

In Abhängigkeit vom Beobachtungsvektor $$X$$ bestimme man $$\hat\gamma = (\hat{\gamma}_0,\hat{\gamma}_1)^\transp,$$ so dass der mittlere quadratische Fehler
$$
F_\gamma \coloneqq
\frac{1}{n} \sum_{k=1}^n (X_k - (\gamma_0 + \gamma_1 t_k))^2 =
\frac{1}{n} \| X - \gamma_0 \vec{1} - \gamma_1 t \|^2
$$
für $$\gamma = \hat{\gamma}$$ minimal ist. Hier ist $$\vec{1} = (1,\dots,1)^\transp \in \R^n.$$

@@.theorem
''Satz (Regressionsgerade).''
Die eindeutigen kleinste-Quadrate-Schätzer sind gegeben durch die Statistiken
$$
\hat\gamma_0 = \bar{X} - \frac{C(t,X)}{V(t)} \bar{t},\qquad
\hat\gamma_1 = \frac{C(t,X)}{V(t)}.
$$
Mit "Varianz" und "Kovarianz"
$$
V(t) = \frac{1}{n} \sum_{i=1}^n t_i{}^2 - \bar{t}\,^2,\qquad
C(t,X) = \frac{1}{n} \sum_{i=1}^n t_i X_i - \bar{t}\bar{X}.
$$
Die Schätzer sind außerdem erwartungstreu.
@@